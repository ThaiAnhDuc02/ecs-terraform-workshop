[
{
	"uri": "/7-cicd-github/1-template/",
	"title": "Clone GitHub Template",
	"tags": [],
	"description": "",
	"content": " In this section, we provide a sample template to run CI/CD using GitHub Actions. You can refer to and clone this to simplify your setup, making the CI/CD deployment process faster and more efficient. Alternatively, you can configure your own CI/CD section to manage your code more easily.\nClone Source Code from GitHub You will reuse the EC2 virtual machine from the previous section to clone this code and edit it there. Once you have the code, you can modify it as needed to suit your requirements.\nUse the following command to check and clone the code. Then navigate into the folder to edit the code as needed. git -v git clone https://github.com/LyHoangViet/cicd-github-action-ws17.git ls cd ./cicd-github-action-ws17/ Edit the source code in the next step to create an example of how a CI/CD run in GitHub Actions will be changed.\nPath: frontend/src/components/Menu.jsx On line 68 in the file, you can edit as shown below (or customize as needed). Code Explanation First, the main.yml file contains all tasks for the frontend and backend.\nHow to access the file: vi .github/workflows/main.yml name: CI/CD Pipeline on: push: tags: - \u0026#39;*\u0026#39; jobs: frontend: uses: ./.github/workflows/frontend.yml secrets: inherit backend: uses: ./.github/workflows/backend.yml secrets: inherit Explanation:\nname: CI/CD Pipeline specifies the workflow structure. on: (\u0026hellip;) defines the event that triggers the workflow. jobs: (\u0026hellip;) lists the jobs that will run when CI/CD is executed. Next, the backend.yml and frontend.yml files configure specific jobs for these sections. Since the configurations are similar, we\u0026rsquo;ll focus on the backend.yml file.\nHow to access the file: vi .github/workflows/backend.yml name: Backend CI/CD on: workflow_call: # Use Docker Hub env: IMAGE_NAME_BE: ${{ secrets.DOCKER_USERNAME }}/backend:${{ github.ref_name }} jobs: # this comment need when you implement in prod env (1) # check_changes_backend: # runs-on: ubuntu-latest # outputs: # backend: ${{ steps.filter.outputs.backend }} # steps: # - uses: actions/checkout@v2 # - uses: dorny/paths-filter@v2 # id: filter # with: # filters: | # backend: # - \u0026#39;backend/**\u0026#39; build-and-push: # this comment need when you implement in prod env (1) # needs: check_changes_backend # if: ${{ needs.check_changes_backend.outputs.backend == \u0026#39;true\u0026#39; }} runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: ${{ secrets.AWS_REGION }} # Use Docker Hub - name: Login to Docker Hub uses: docker/login-action@v2 with: username: ${{ secrets.DOCKER_USERNAME }} password: ${{ secrets.DOCKER_PASSWORD }} - name: Build, tag, and push image to Amazon ECR working-directory: ./backend run: | docker build -f Dockerfile -t ${{ env.IMAGE_NAME_BE }} . docker push ${{ env.IMAGE_NAME_BE }} deploy: needs: build-and-push runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: ${{ secrets.AWS_REGION }} - name: Deploy Backend env: TASK_NAME_BE: ${{ secrets.TASK_NAME_BE }} IMAGE_NAME_BE: ${{ env.IMAGE_NAME_BE }} AWS_APPLICATION_NAME: ${{ secrets.AWS_APPLICATION_NAME }} AWS_DEPLOYMENT_GROUP_NAME: ${{ secrets.AWS_DEPLOYMENT_GROUP_NAME }} REGION: ${{ secrets.AWS_REGION }} run: bash backend/deploy-backend.sh Explanation: name: Backend CI/CD is the workflow name. on: (\u0026hellip;) triggers the workflow, allowing it to call other workflows. env: (\u0026hellip;) defines environment variables for the image (currently using Docker Hub, but can be switched to ECR). build-and-push: (\u0026hellip;) is a job within the main jobs section, used to build an image and push it to Docker Hub. deploy: (\u0026hellip;) is the second job, used to deploy the image to ECS. run: bash backend/deploy-backend.sh points to the deployment file. Finally, access the deploy-backend.sh file (or deploy-frontend.sh for the frontend) to configure code for ECS deployment.\nHow to access the file: vi backend/deploy-backend.sh #!/bin/bash # Retrieve the task definition from ECS TASK_DEFINITION=$(aws ecs describe-task-definition --task-definition \u0026#34;$TASK_NAME_BE\u0026#34; --region \u0026#34;$REGION\u0026#34;) # Register a new task definition with an updated image NEW_TASK_DEFINITION=$(echo $TASK_DEFINITION | jq --arg IMAGE \u0026#34;$IMAGE_NAME_BE\u0026#34; \u0026#39;.taskDefinition | .containerDefinitions[0].image = $IMAGE | del(.taskDefinitionArn) | del(.revision) | del(.status) | del(.requiresAttributes) | del(.compatibilities) | del(.registeredAt) | del(.registeredBy)\u0026#39;) NEW_REVISION=$(aws ecs register-task-definition --region \u0026#34;$REGION\u0026#34; --cli-input-json \u0026#34;$NEW_TASK_DEFINITION\u0026#34;) echo $NEW_REVISION | jq \u0026#39;.taskDefinition.revision\u0026#39; # Extract the task definition ARN, container name, and container port using jq AWS_TASK_DEFINITION_ARN=$(echo $TASK_DEFINITION | jq -r \u0026#39;.taskDefinition.taskDefinitionArn\u0026#39;) CONTAINER_NAME=$(echo $TASK_DEFINITION | jq -r \u0026#39;.taskDefinition.containerDefinitions[0].name\u0026#39;) CONTAINER_PORT=$(echo $TASK_DEFINITION | jq -r \u0026#39;.taskDefinition.containerDefinitions[0].portMappings[0].containerPort\u0026#39;) # Create the application specification (AppSpec) in JSON format APP_SPEC=$(jq -n --arg taskDef \u0026#34;$AWS_TASK_DEFINITION_ARN\u0026#34; --arg containerName \u0026#34;$CONTAINER_NAME\u0026#34; --argjson containerPort \u0026#34;$CONTAINER_PORT\u0026#34; \u0026#39;{ version: \u0026#34;0.0\u0026#34;, Resources: [ { TargetService: { Type: \u0026#34;AWS::ECS::Service\u0026#34;, Properties: { TaskDefinition: $taskDef, LoadBalancerInfo: { ContainerName: $containerName, ContainerPort: $containerPort } } } } ] }\u0026#39;) # Create the revision configuration for the deployment REVISION=$(jq -n --arg appSpec \u0026#34;$APP_SPEC\u0026#34; \u0026#39;{ revisionType: \u0026#34;AppSpecContent\u0026#34;, appSpecContent: { content: $appSpec } }\u0026#39;) # Trigger the deployment using the specified application name, deployment group, and revision aws deploy create-deployment --region \u0026#34;$REGION\u0026#34; \\ --application-name \u0026#34;$AWS_APPLICATION_NAME\u0026#34; \\ --deployment-group-name \u0026#34;$AWS_DEPLOYMENT_GROUP_NAME\u0026#34; \\ --revision \u0026#34;$REVISION\u0026#34; Results: The ECS task definition is updated with the new image. CodeDeploy triggers a deployment with the updated configuration, synchronizing the application with the new backend image in the existing ECS service. "
},
{
	"uri": "/5-ecs-service/1-backend/",
	"title": "Deploying Blue/Green and Service Scaling with Backend",
	"tags": [],
	"description": "",
	"content": "Creating ECS Service Now, we need to create an ECS service to run our containers.\nSearch for the keyword ECS in the AWS Console. Select the Cluster that you\u0026rsquo;ve created. Scroll down to the Service section and choose Create. We’ll configure FARGATE for easier usage.\nChoose Launch type as FARGATE. Version select LATEST. Choose type as Service. Family select the Task definition created for the backend and choose the latest version. Enter name: backend Choose type as Replica and select 1 task to launch. Select deployment type as Blue/Green. Select deployment configuration as CodeDeployDefault.ECSCanary10Percent5Minutes. In the Role section, choose the role created in the preparation step: CodeDeployServiceRole. Service Discovery This part is important. For the Frontend Service and Backend Service to communicate, we need to configure Service discovery for the Backend Service with the namespace and service created in Cloud Map (configured with Terraform).\nSelect Use service discovery. Choose Select an existing namespace and select the namespace fcjresbar.internal that was created previously. Choose Select an existing service discovery service and select the backend service. Leave other configurations as default. Networking Now, we’ll assign this Service and its containers to the network and subnet that we previously created using Terraform.\nVPC: Choose the VPC that we created earlier. Subnet: Choose the private subnet (DoAn-network-subnet-private4) created in the preparation step. Security group: Select DoAn-network-sg-private. Public IP: This option will be disabled to enhance security as traffic is already routed through the Load Balancer. Load Balancing Configure some information for the Load Balancer as follows:\nLoad balancing type: Choose Application Load Balancer. Container: backend 5000:5000 (this port is for both the host and the container). Choose Use an existing load balancer. Select Doan-alb load balancer. Health Check: Keep it as default. In the Listener section, we’ll create 2 new Listeners.\nProduction listener: port 5000, protocol HTTP. Test listener: port 8080, protocol HTTP. The target group section can be kept as default or configured as desired.\nService Auto Scaling Now, we’ll configure scaling for the container.\nMinimum: 1 Maximum: 2 Scaling policy type: Target tracking Configure the scaling policy:\nName: ScaleAtThreshold75Percent ECS service metric: ECSServiceAverageCPUUtilization Target value: 75 Scale-out cooldown period: 120 Scale-in cooldown period: 120 Then click Create to finish. We need to wait about 5 minutes for the Service to be fully created.\n"
},
{
	"uri": "/",
	"title": "Infrastructure Deployment with Terraform Integrated with GitHub Actions",
	"tags": [],
	"description": "",
	"content": "Infrastructure Terraform Integrated with GitHub Actions Architecture Workflow To complete this Workshop, we will follow a simple process as outlined below:\nWrite the Terraform infrastructure code for the pre-defined system. Use commands to deploy the infrastructure on the AWS console. Deploy ECS Service on the pre-created infrastructure. Set up CI/CD to automate the deployment process. Create Monitoring to observe and manage the system. This process ensures that the deployment of infrastructure, services, and CI/CD is fully automated, providing high efficiency and comprehensive control over the AWS-based system.\nTable of Contents Introduction Preparation Terraform Infrastructure Add Database to RDS Create ECS Service Verify Results CI/CD Deployment with GitHub Actions Monitoring Resource Cleanup "
},
{
	"uri": "/3-terraform/1-introduce/",
	"title": "Infrastructure Introduction",
	"tags": [],
	"description": "",
	"content": "Terraform Infrastructure As previously introduced, Terraform allows for the automated and organized configuration of cloud infrastructure on AWS. The template prepared below already includes code for infrastructure deployment, with the structure outlined as follows:\nThere are 4 main files used throughout the configuration and setup of each service:\nterraform.tf: Configures the provider, which is a basic file and typically requires minimal changes. variable.tf: Stores variables, making it easy to adjust values for each deployment. main.tf: The main file to run programs and services for the configured infrastructure. output.tf: Outputs the values of resources post-deployment, allowing these values to be reused for other purposes. The modules folder will contain the configuration of each service.\nThe .tf files outside the modules folder combine these services, enabling synchronized deployment and optimizing the process.\nTemplate link: https://github.com/ThaiAnhDuc02/terraform-ecs-infra.git\nTo better understand the sections in the provided template, I recommend reviewing the Terraform course on KodeKloud and creating your own infrastructure to gain additional insight into this template.\nYou can read the README file to learn more about this infrastructure. Install the necessary tools as instructed. "
},
{
	"uri": "/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Context Why this Workshop?\nIn practice, deploying AWS infrastructure with Containers can often be complex and time-consuming. Manual processes are prone to configuration errors, difficult to manage, and can lead to unnecessary resource waste when issues arise. Terraform addresses these challenges by automating the creation and management of infrastructure, making the process faster and easier. Combined with CI/CD, deployment becomes more efficient and trackable, helping businesses reduce risk and improve system management effectiveness.\nThis process offers clear benefits:\nTime and Cost Savings: Automation reduces manual work and increases work efficiency. High Reliability: Services are deployed consistently, minimizing configuration errors. Flexible Scalability: The infrastructure can easily scale up or down according to demand, optimizing operational costs. Quick Response to Changes: CI/CD enables faster product improvement and system updates, better meeting market demands. Thanks to this process, the company not only meets expansion needs but also optimizes service quality, increasing competitiveness and enhancing customer satisfaction.\nWhat is Terraform? Terraform is an open-source tool developed by HashiCorp used for managing infrastructure (Infrastructure as Code - IaC). It enables users to define and deploy infrastructure via code, automating the creation, updating, and management of resources such as servers, databases, networks, and other services on cloud platforms like AWS, Azure, Google Cloud, as well as on-premises systems.\nKey features of Terraform:\nInfrastructure as Code (IaC): Infrastructure is described in code, allowing for version control, easy sharing, and backup, similar to managing an application\u0026rsquo;s source code. Automated Deployment and Configuration: With Terraform, users can deploy or change resources in bulk with a single command, saving time and reducing manual errors. Platform Independence: Terraform supports multiple cloud providers, making it easy to deploy multi-cloud infrastructure or switch between platforms without changing tools. Lifecycle Management of Resources: Terraform not only creates but also manages the entire resource lifecycle, including updates, modifications, and deletions as needed. Modularity and Extensibility: Terraform allows infrastructure to be divided into modules, making management flexible, easy to maintain, and scalable as the system grows. With these advantages, Terraform simplifies infrastructure management, making changes safe, controllable, and easy to deploy on a large scale.\nGitHub Actions CI/CD GitHub Actions is an integrated automation service on GitHub, helping to automate the software development workflow from code testing to application deployment, making it easy to set up CI/CD (Continuous Integration/Continuous Deployment).\nContinuous Integration (CI) Automatic Build and Test: Automatically builds and tests code whenever there are changes, helping to detect errors early. Workflows: Defines workflows to automatically perform testing, building, and releasing code on events like new pull requests or commits. Continuous Deployment (CD) Automated Deployment: Supports automatic application deployment to environments like staging or production when code is approved. Service Integration: Easily integrates with services like AWS, Azure, and Google Cloud. Key Features Customization: Create custom actions or use actions from the GitHub Marketplace. User-Friendly Interface: Easily set up and manage workflows without deep scripting knowledge. Resource Management: Monitors workflow status and provides detailed information about the testing and deployment process. Benefits Time-Saving: Reduces repetitive work for developers. Improved Code Quality: Enhances software quality through frequent testing and deployment. Flexibility: Easily adjust CI/CD processes to suit project needs. Monitoring with CloudWatch Container Insights CloudWatch Container Insights is a feature in AWS CloudWatch that provides monitoring and analytics capabilities for containers running on Amazon ECS (Elastic Container Service) and EKS (Elastic Kubernetes Service). Below are some highlights of Container Insights:\nPerformance Monitoring: Container Insights allows you to track key performance metrics such as CPU, memory, and network for each container, helping to detect issues early and optimize application performance. Container Health Statistics: You can view detailed information about the operational status of containers, including their state (running, stopped, error) and other metrics like runtime and replica count. AWS Integration: Container Insights can be easily integrated with other AWS services, allowing you to build comprehensive monitoring solutions and create custom reports based on container metrics. Visual Dashboard: AWS provides visual dashboards that make it easy to monitor the status and performance of containers, along with the ability to set alerts when issues arise. Before starting the Workshop, we recommend spending a bit of time getting familiar with the contents you’ll be working on. Now, let’s dive in and explore each step of the Workshop. Wishing you a smooth experience and the best results!\n"
},
{
	"uri": "/2-preparation/1-dependencies/",
	"title": "Prepare Dependencies",
	"tags": [],
	"description": "",
	"content": "Core Knowledge To better prepare for this workshop, you should review three exercises from the AWS Study Group:\nDeploy Application on Docker Container. Deploy applications on Amazon Elastic Container Service. Deploying CI/CD with ECS Container Completing these exercises will help you gain a strong understanding of:\nHow to deploy infrastructure on AWS. Using RDS for database management. Creating ECS to run containers for applications. Using an Application Load Balancer to distribute load evenly. Setting up CI/CD to automate application updates and deployment. Monitoring and tracking process performance. Once you’ve mastered these concepts, you’ll be ready to deploy infrastructure quickly with just a few lines of code using Terraform. Now, let’s dive into the workshop and explore each step together!\nInstalling Dependencies To start running code in Terraform, you’ll need:\nTerraform CLI: This command-line tool helps you manage and deploy infrastructure as code. AWS CLI: This tool allows Terraform to interact directly with AWS services, making it easy to manage cloud resources. Select the version compatible with your operating system. You can follow the detailed installation guide in the video below.\n"
},
{
	"uri": "/7-cicd-github/2-push-code/",
	"title": "Create a New Project and Push Code",
	"tags": [],
	"description": "",
	"content": "Create a New Repository This repository will be used to run GitHub Actions.\nFirst, log into your GitHub account. Select Repositories. Choose New. Enter the repository name: terraform-cicd Enter a description: Use for CI/CD Select Public. Click Create repository. Repository creation is complete. Push Code Access the virtual machine where you have cloned and edited your code.\nUse the command to set up remote repo. Check if remote setup was successful. git remote set-url origin \u0026#34;GitHub repo link\u0026#34; git remote -v Use the following commands to push your code to the newly created repository. git add . git commit -m \u0026#34;Fist commit\u0026#34; git branch -M main git push -u origin main Verify in the GitHub repository to see if the code has been successfully pushed. "
},
{
	"uri": "/3-terraform/2-git/",
	"title": "Git clone template",
	"tags": [],
	"description": "",
	"content": "Clone template As noted in the Preparation section, you need to have the required dependencies installed on your machine, and you can use Visual Studio Code for easier folder management.\nUse the command below to check if Git is installed. git -v Next, clone the repository to run the code. git clone https://github.com/ThaiAnhDuc02/ecs-terraform.git cd .\\Terraform-DoAn\\deploy-infrastructure-ecs\\ Log in to AWS CLI aws --version aws configure "
},
{
	"uri": "/2-preparation/",
	"title": "Preparation",
	"tags": [],
	"description": "",
	"content": "Content Prepare Dependencies Prepare Elastic Container Registry Create Access key, Role and Key pair "
},
{
	"uri": "/2-preparation/2-ecr/",
	"title": "Prepare the Elastic Container Registry",
	"tags": [],
	"description": "",
	"content": "Prepare repository on Amazon ECR In this section, we will create and prepare ECR repositories. If you are unfamiliar with pushing images to Amazon ECR, please refer to [AWS documentation on Amazon ECR](https://docs.aws.amazon.com/AmazonECR/latest/userguide/getting-started-console. html) for detailed instructions on the process and necessary steps.\nCreate 2 repositories named frontend and backend. "
},
{
	"uri": "/5-ecs-service/2-frontend/",
	"title": "Rolling Deployment with Frontend",
	"tags": [],
	"description": "",
	"content": "Create ECS Frontend Service Similar to the Backend Service, we now need to create the Frontend Service.\nChoose Launch type as FARGATE. Set Version to LATEST. Select type as Service. For Family, choose the Task definition created for the frontend and select the latest version. Enter name: frontend Set type as Replica and select 1 task to launch. Unlike the backend, the frontend will run by default in Rolling mode. Networking Now, we will assign this Service and its containers to the network area and subnet we previously created using Terraform.\nVPC: Select the VPC created earlier. Subnet: Select the private subnet (DoAn-network-subnet-private3) created in the preparation phase. Security group: Select DoAn-network-sg-private. Public IP: Disable this setting to enhance security since traffic is already routed through the Load Balancer. Load Balancing Configure some information for the Load Balancer as follows:\nLoad balancing type: Choose Application Load Balancer. Container: Select frontend 80:80 (port here refers to the host and container port). Choose Use an existing load balancer. Select Doan-alb load balancer. Health check grace period: Set to 30. In the listener section, we’ll select the listener created earlier in Terraform.\nChoose Use an existing listener and select 80:HTTP. For the target group, select the target group previously created in Terraform.\nChoose Use an existing target group and select my-tg. Review the configuration and select Create.\nWait approximately 5 minutes for the service to be fully created. "
},
{
	"uri": "/7-cicd-github/3-secret-key/",
	"title": "Add Secret Keys",
	"tags": [],
	"description": "",
	"content": "Create Secret Keys in GitHub Create secret keys in GitHub to assign these values to variables in the CI/CD pipeline, ensuring it runs correctly.\nGo to the Settings of the repository you created in the previous section. Select Secrets and variables and Actions as shown. Click New repository secret to create a new secret. Add the following values as shown below:\nAWS_ACCESS_KEY_ID: Add the access key value you created in AWS. AWS_SECRET_ACCESS_KEY: Add the secret key value you created in AWS. AWS_REGION: Set this to the region you are using. AWS_ACCOUNT_NUMBER: Set this to the account number you are using. DOCKER_PASSWORD: Add the password or token from your Docker account. DOCKER_USERNAME: Add the username from your Docker account. CLUSTER_NAME: Set this to the name of your ECS Cluster. TASK_NAME_BE: Set this to the name of the backend task definition. TASK_NAME_FE: Set this to the name of the frontend task definition. SERVICE_NAME_BE: Set this to the name of the backend service. SERVICE_NAME_FE: Set this to the name of the frontend service. AWS_APPLICATION_NAME: Set this to the name of the Application in CodeDeploy for the backend. AWS_DEPLOYMENT_GROUP_NAME: Set this to the name of the Deployment Group in the backend Application. Since this repository is using Docker Hub to store images, if you wish to use a different registry like ECR, you will need to adjust the variables in the configuration files to match ECR.\n"
},
{
	"uri": "/3-terraform/3-variable/",
	"title": "Edit Variables",
	"tags": [],
	"description": "",
	"content": "Explanation of Variables To better understand the variables in this Terraform infrastructure, you can review the code snippet below along with detailed annotations.\n# Setup local variables locals { region = \u0026#34;ap-southeast-1\u0026#34; author = \u0026#34;DUKE\u0026#34; network_root_name = \u0026#34;FCJ-network\u0026#34; vpc_cidr = \u0026#34;10.0.0.0/16\u0026#34; compute_root_name = \u0026#34;FCJ-compute\u0026#34; key_name = \u0026#34;FCJ-Lab-key\u0026#34; # RDS database db_username = \u0026#34;admin\u0026#34; db_password = \u0026#34;letmein12345\u0026#34; db_name = \u0026#34;fcjdb\u0026#34; # Cloud Map service_discovery_namespace_name = \u0026#34;fcjresbar.internal\u0026#34; service_discovery_service_name = \u0026#34;backend\u0026#34; # Load Balancer target_group_name = \u0026#34;my-tg\u0026#34; alb_name = \u0026#34;FCJ-alb\u0026#34; # Task definition of backend backend_family = \u0026#34;fcjresbar-task-be\u0026#34; backend_image = \u0026#34;730335321184.dkr.ecr.ap-southeast-1.amazonaws.com/backend-image\u0026#34; mysql_database = \u0026#34;fcjresbar\u0026#34; db_dialect = \u0026#34;mysql\u0026#34; be_port = \u0026#34;5000\u0026#34; jwt_secret = \u0026#34;0bac010eca699c25c8f62ba86e319c2305beb94641b859c32518cb854addb5f4\u0026#34; # Task definition of frontend frontend_family = \u0026#34;fcjresbar-task-fe\u0026#34; frontend_image = \u0026#34;730335321184.dkr.ecr.ap-southeast-1.amazonaws.com/frontend-image\u0026#34; be_host = \u0026#34;backend.fcjresbar.internal\u0026#34; ec2_instances = [ { name = \u0026#34;server_test\u0026#34; ami = \u0026#34;ami-06650ca7ed78ff6fa\u0026#34; # Ubuntu Server 24.04 LTS instance_type = \u0026#34;t2.medium\u0026#34; subnet_id = module.infrastructure_vpc.subnet_public1_id security_group_ids = [module.security.public_sg_id] }, ] } Note: To function correctly, the above variables should be modified according to your specific configuration parameters.\nPath to code vi .\\deploy-infrastructure-ecs\\terraform.tf "
},
{
	"uri": "/3-terraform/",
	"title": "Terraform Infrastructure",
	"tags": [],
	"description": "",
	"content": "Content Introduce infrastructure Git clone template Edit variable Run command Terraform "
},
{
	"uri": "/4-database/",
	"title": "Add Database to RDS",
	"tags": [],
	"description": "",
	"content": "Connect to EC2 Instance First, I need to access the instance created by Terraform and connect to it to add data to RDS.\nSearch for ECS Instance in the AWS console. Select the name of the instance created and click Connect. Choose SSH client. Copy the path as shown. Paste it into your VS Code and configure the path for the Key pair you saved. Successfully connected. Add Database Since my Terraform configuration has already set up the dependencies and includes a sample database, adding data to RDS will be easier.\nNow, we will navigate to the pre-set directory and add the absolute path for the database here.\nWe will use the following command: cd ./aws-fcj-container-app/database/ echo $PWD/init.sql Next, we need to log in to RDS to add the data from that path.\nCopy the RDS Endpoint. Use that endpoint in the following command: mysql -h \u0026#34;rds-endpoint\u0026#34; -u \u0026#34;name-user\u0026#34; -p Paste the command into the virtual machine and enter the password to connect to RDS. Paste the path with data into the connected RDS. source /home/ubuntu/aws-fcj-container-app/database/init.sql Data added successfully. Use some commands to check the database. SHOW DATABASES; USE fcjresbar; SELECT * FROM Clients; "
},
{
	"uri": "/3-terraform/4-command/",
	"title": "Run Terraform Command",
	"tags": [],
	"description": "",
	"content": "Run Command Now, I will run the command to deploy the configured infrastructure on AWS.\nRun the command to initialize the infrastructure. terraform init Use the next command to preview the changes that will be applied to the infrastructure. terraform plan The final command is used to apply the configured infrastructure to AWS. terraform apply Type yes. Wait about 15 to 20 minutes for the infrastructure to be fully created. Check the Results After the process is complete, we will go to the AWS console to check the interface to see if the infrastructure has been fully created.\nCheck inside VPC. Check in EC2 instance. Access RDS. Check ECS cluster. Check Task definition. Check the services in Cloud Map. "
},
{
	"uri": "/7-cicd-github/4-result/",
	"title": "Check Results",
	"tags": [],
	"description": "",
	"content": "Create a Tag To trigger the CI/CD pipeline, we need to create a new tag for the code.\nGo to your repository. Select Tags. Choose Releases. Click Draft a new release. Enter a new tag name: v1.0.4. Click Create new tag. Select Publish release to confirm. Check Results To monitor the CI/CD process, follow these steps:\nGo to Actions. Select the name of your workflow. View the CI/CD process for both backend and frontend here. If there is an error, check the logs below. After CI/CD has successfully run, go to the AWS Console to check if components such as Task Definitions and Services have been updated.\nGo to Task definitions to see if a new task has been created. First, check the Backend task. Next, check the frontend task. Verify that the Service is being updated. Once everything is complete, go back to the DNS of the Load Balancer to check if your updates have been applied.\nReload the current page. "
},
{
	"uri": "/5-ecs-service/",
	"title": "Create ECS Service",
	"tags": [],
	"description": "",
	"content": "Content Blue/Green deployment and Service Scaling with Backend Deploy Rolling with Frontend "
},
{
	"uri": "/6-result/",
	"title": "Check Results",
	"tags": [],
	"description": "",
	"content": "Check the Application To check the application, you need to obtain the domain from the Load Balancer.\nSearch for Load Balancers in the AWS Console. Select the Load Balancer created in the Terraform infrastructure. Copy the DNS name of the created Load Balancer. Paste the DNS link into a browser to launch the application. Log in and test the functionality of each part. Congratulations on successfully deploying the application. "
},
{
	"uri": "/7-cicd-github/",
	"title": "Deploy CI/CD with GitHub Action",
	"tags": [],
	"description": "",
	"content": "Content Clone GitHub template Create a new project and push code Add Secret key Check result "
},
{
	"uri": "/8-monitoring/",
	"title": "Monitoring",
	"tags": [],
	"description": "",
	"content": "Configure Container Insights (CloudWatch) In this section, we’ll need to configure some settings before using Container Insights. Container Insights is a service that allows us to monitor metrics such as CPU usage, Network activity, and more, giving insight into user activity over a certain period.\nCombined with CloudWatch, we can view logs sent from Containers in groups, usually with the prefix /ecs/*. This gives a more comprehensive view of applications in the system—how they\u0026rsquo;re running, any alerts, and the ability to read logs for troubleshooting errors or investigating unusual traffic when the system is compromised.\nOn the ECS console page, in the right-hand menu, select Account settings. Choose Update. Select Container Insights. Click Save changes. Complete the Container Insights configuration. Observing Metrics with Performance Monitoring Go back to Clusters. Select Metrics. Click on View Container Insights to observe metrics. View the metrics with ECS. To switch to observing Task definitions, follow these steps:\nIn the highlighted section, change it to ECS Tasks to monitor events there. Similarly, to view events for ECS Service:\nIn the highlighted section, change it to ECS Service to monitor those events. Observing Metrics with Container Map Here, you can view which Containers, Tasks, and Services are running within the Cluster you’ve deployed, with greater specificity.\nChange Performance Monitoring to Container Map. You’ll see a new interface showing how a Cluster manages related resources like Services and Tasks. Note that metrics within a Task differ from those within a Service, as Services directly manage Tasks, meaning Task metrics are also applicable to Services.\nHere, you can also view details for each service.\nClick on Service backend. Select Metrics. Similarly, you can view Task definitions:\nClick on Task fe. Choose Metrics. Observing Metrics with Resources Change Container Map to Resources. Here, you’ll see a list of created events, allowing you to select and monitor each service.\nWe have completed configuration and monitoring with Container Insights, but AWS provides other monitoring and tracking services beyond Container Insights, including third-party services.\nCongratulations on completing this Workshop!\n"
},
{
	"uri": "/9-clean-up/",
	"title": "Resource Cleanup",
	"tags": [],
	"description": "",
	"content": "Cleaning Up on the AWS Console First, you need to access the AWS console to delete the services you previously created manually.\nIn the console, search for CloudFormation. Select Stacks. Choose the 2 Services you created and click Delete. Click Delete. Wait approximately 5 to 10 minutes for the deletion process to complete. Go back to the ECS cluster to check if the service has been deleted. Cleaning Up with Terraform Commands Since the resources in the infrastructure were created by Terraform, you can conveniently delete all resources with a single command instead of manually removing each one.\nNow, go back to the location where you initially deployed the infrastructure with Terraform.\nUse the command below to delete resources. terraform destroy Type yes to confirm. Wait approximately 15 to 20 minutes for all resources to be deleted. After a successful deletion, return to the AWS console to verify that all resources have been removed. "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]